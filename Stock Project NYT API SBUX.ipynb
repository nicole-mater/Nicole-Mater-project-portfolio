{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c97916-180c-49b0-89d3-4c8b0361d31b",
   "metadata": {},
   "source": [
    "## Starbucks NYT Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b6669a-8f1e-4d3d-972d-7f4c70566789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca77fd4-fe8d-47b2-854b-a0acc41cc92c",
   "metadata": {},
   "source": [
    "### Requesting from the NYT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe186b3-2f68-447c-b8bd-987de206294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished year 2020: no more articles at page 28\n",
      "Finished year 2021: no more articles at page 28\n",
      "Finished year 2022: no more articles at page 40\n",
      "Finished year 2023: no more articles at page 27\n",
      "Finished year 2024: no more articles at page 25\n",
      "Finished year 2025: no more articles at page 17\n"
     ]
    }
   ],
   "source": [
    "# pull all articles relevant to Starbucks in our time frame\n",
    "\n",
    "years = range(2020, 2026) # Nov-15-2020 start date to match Google Trends data\n",
    "allresults = []\n",
    "baseurl=\"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "apikey=\"GGCXEfbqfbBKk5Aii36FMKPK6De5udGe\"\n",
    "\n",
    "# for loop to iterate through each year in our time frame\n",
    "for year in years:\n",
    "    begin = f\"{year}0101\" # changes the year for the api call for each loop\n",
    "    end   = f\"{year}1231\"\n",
    "\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        pa1={\"api-key\":apikey,\n",
    "            \"q\": '\"Starbucks\"',\n",
    "            \"begin_date\": begin,\n",
    "            \"end_date\": end,\n",
    "            \"page\": page}\n",
    "\n",
    "        sbuxurl = baseurl + urllib.parse.urlencode(pa1)\n",
    "        request1 = urllib.request.urlopen(sbuxurl).read()\n",
    "        resd1 = json.loads(request1)\n",
    "\n",
    "        response_block1 = resd1.get(\"response\")\n",
    "    \n",
    "        # failsafe troubleshooting for API error\n",
    "        if response_block1 is None:\n",
    "            print(f\"API returned no response for year {year}, page {page}. Retrying...\")\n",
    "            time.sleep(18)\n",
    "            continue   # try again\n",
    "    \n",
    "        docs = response_block1.get(\"docs\")\n",
    "        \n",
    "        # indicates end of articles\n",
    "        if docs is None or len(docs) == 0:\n",
    "            print(f\"Finished year {year}: no more articles at page {page}\")\n",
    "            break\n",
    "\n",
    "        # collect the objects of interest\n",
    "        for doc in docs:\n",
    "            allresults.append({\n",
    "                \"headline\"         : doc.get('headline', {}).get('main', ''),\n",
    "                \"abstract\"         : doc.get('abstract', ''),\n",
    "                \"publication_date\" : doc.get('pub_date', ''),\n",
    "                \"document_type\"    : doc.get('document_type', ''),\n",
    "                \"section_name\"     : doc.get('section_name', ''),\n",
    "                \"subsection_name\"  : doc.get('subsection_name', '')\n",
    "            })\n",
    "       \n",
    "        time.sleep(18)  # time delay so we don't hit the article limit\n",
    "        page += 1  # scraping every page of articles\n",
    "        \n",
    "sbuxdf = pd.DataFrame(allresults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a80cd8ac-d375-4846-b3e2-b8f1b5f3a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                headline  \\\n",
       "0     Starbucks sets goals to increase diversity thr...   \n",
       "1     Starbucks Will Allow Employees to Wear Black L...   \n",
       "2     Starbucks names Mellody Hobson as its board ch...   \n",
       "3     Starbucks sales plunge 40 percent even as stor...   \n",
       "4     Starbucks Barista Gets $87,000 in Donations Af...   \n",
       "...                                                 ...   \n",
       "1597            Teenagers on How Covid Has Changed Them   \n",
       "1599  How David Henry Hwang Remade Theater in His Ow...   \n",
       "1602  My Parents Expected to Be Retired. Instead, Th...   \n",
       "1603                  Nothing Lasts. How Do We Face It?   \n",
       "1604  Inside the Improbable, Audacious and (So Far) ...   \n",
       "\n",
       "                                               abstract      publication_date  \\\n",
       "0                                                        2020-10-14T16:22:39Z   \n",
       "1     In announcing the reversal, the coffee chain a...  2020-06-12T20:23:22Z   \n",
       "2                                                        2020-12-09T19:28:41Z   \n",
       "3                                                        2020-07-28T22:31:49Z   \n",
       "4     A woman who was not wearing a mask griped on F...  2020-06-27T21:59:53Z   \n",
       "...                                                 ...                   ...   \n",
       "1597  This month marks the fifth anniversary of the ...  2025-03-20T19:31:55Z   \n",
       "1599  Long the leading Asian American playwright, he...  2025-10-09T09:01:42Z   \n",
       "1602  My mom and dad joined the millions of American...  2025-05-18T09:00:15Z   \n",
       "1603  Transience has come to inform so much of Japan...  2025-04-22T09:01:40Z   \n",
       "1604  The story of the man most likely to be the nex...  2025-10-14T09:01:15Z   \n",
       "\n",
       "     document_type          section_name subsection_name  \n",
       "0          article              Business                  \n",
       "1          article              Business                  \n",
       "2          article              Business                  \n",
       "3          article              Business                  \n",
       "4          article              Business                  \n",
       "...            ...                   ...             ...  \n",
       "1597       article  The Learning Network                  \n",
       "1599       article            T Magazine                  \n",
       "1602       article              Magazine                  \n",
       "1603       article            T Magazine                  \n",
       "1604       article              Magazine                  \n",
       "\n",
       "[1555 rows x 6 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbuxdf = sbuxdf[\n",
    "    (sbuxdf['document_type'] == \"article\") ] # filtering the results down to article types only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044f770-2d6f-401a-9f2d-afe5b91865d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbuxdf = sbuxdf[\n",
    "    sbuxdf['headline'].str.contains(\"Starbucks\", case=False, na=False) |\n",
    "    sbuxdf['abstract'].str.contains(\"Starbucks\", case=False, na=False)]\n",
    "# filtering the output so the headline or abstract include the company name\n",
    "\n",
    "sbuxdf.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ba246-911f-49ab-a38a-55edd51c0b5a",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506bb6a7-c29c-4aa4-9662-53b4793ba1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# combine the headline and abstract into one field for sentiment analysis\n",
    "sbuxdf['text'] = sbuxdf['headline'] + \" \" + sbuxdf['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a61e769-e6e6-4a77-b826-f78f4f84806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pos = []\n",
    "sent_neg = []\n",
    "sent_neu = []\n",
    "sent_comp = []\n",
    "corpus = sbuxdf['text']\n",
    "\n",
    "# iterate through each sentence in corpus\n",
    "for article in corpus:\n",
    "    \n",
    "    # analyze the sentiment. ss is a dictionary\n",
    "    ss = sia.polarity_scores(article)\n",
    "\n",
    "    # append and store these sentiment scores into a list\n",
    "    sent_pos.append(ss['pos'])\n",
    "    sent_neg.append(ss['neg'])\n",
    "    sent_neu.append(ss['neu'])\n",
    "    sent_comp.append(ss['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd43fcb8-7b35-4a87-9904-64ac9a6fb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the list to the dataframe as column using assign(column_name = data)\n",
    "sbuxdf = sbuxdf.assign(article_pos = sent_pos)\n",
    "sbuxdf = sbuxdf.assign(article_neg = sent_neg)\n",
    "sbuxdf = sbuxdf.assign(article_neu = sent_neu)\n",
    "sbuxdf = sbuxdf.assign(article_comp = sent_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f4449-a3a9-4f1e-9fd2-e3e53e982cd2",
   "metadata": {},
   "source": [
    "### Summarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70aec6a9-7637-441f-94de-8a48d0e84fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbuxdf = sbuxdf.copy()\n",
    "sbuxdf['publication_date'] = pd.to_datetime(sbuxdf['publication_date'])\n",
    "\n",
    "# find the Friday of the week for each publication date\n",
    "sbuxdf['fiscal_week'] = sbuxdf['publication_date'] + pd.offsets.Week(weekday=4)\n",
    "sbuxdf['fiscal_week'] = sbuxdf['fiscal_week'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "708d8017-48ed-4b94-846f-4add2cf327fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = (\n",
    "    sbuxdf.groupby('fiscal_week')\n",
    "          .agg(\n",
    "               article_count = ('text', 'count'),\n",
    "              pos_sentiment  = ('article_pos', 'sum'),\n",
    "              neg_sentiment  = ('article_neg', 'sum'),\n",
    "              neu_sentiment  = ('article_neu', 'sum'),\n",
    "              comp_sentiment = ('article_comp', 'mean'),\n",
    "          )\n",
    "          .reset_index()\n",
    ")\n",
    "# group the rest of the columns by summing or averaging the values for the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131865f3-c655-434e-97a7-bff4f7cc7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ratios to analyze article sentiment\n",
    "# total\n",
    "weekly_df['total_sent'] = weekly_df['pos_sentiment'] + weekly_df['neg_sentiment'] + weekly_df['neu_sentiment']\n",
    "# ratios\n",
    "weekly_df['pos_ratio'] = weekly_df['pos_sentiment'] / weekly_df['total_sent']\n",
    "weekly_df['neg_ratio'] = weekly_df['neg_sentiment'] / weekly_df['total_sent']\n",
    "\n",
    "weekly_df = weekly_df.drop(columns=['total_sent', 'pos_sentiment', 'neg_sentiment', 'neu_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87b6c79d-1875-48ed-acc6-2f4fe50ee41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of     fiscal_week  article_count  comp_sentiment  pos_ratio  neg_ratio\n",
       "0    2020-01-03              1       -0.214400   0.000000   0.062000\n",
       "1    2020-01-10              4       -0.038875   0.076250   0.069750\n",
       "2    2020-01-17              3        0.002333   0.105000   0.081333\n",
       "3    2020-01-24              5       -0.305260   0.045800   0.107800\n",
       "4    2020-01-31             13        0.029608   0.073615   0.043769\n",
       "..          ...            ...             ...        ...        ...\n",
       "297  2025-10-24              3       -0.117767   0.068000   0.072667\n",
       "298  2025-10-31              6        0.222650   0.091485   0.044993\n",
       "299  2025-11-07              3        0.436167   0.111333   0.000000\n",
       "300  2025-11-14              5        0.476920   0.148630   0.018004\n",
       "301  2025-11-28              1        0.680800   0.159000   0.035000\n",
       "\n",
       "[302 rows x 5 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final dataset\n",
    "weekly_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0d2f43-d589-4059-94c3-6ab783dd4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df.to_csv(\"sbux_nyt_data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
